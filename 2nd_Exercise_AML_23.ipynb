{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf3z5SqWZ91b"
      },
      "source": [
        "# Torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "YbtEmI1AiTkF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "#use GPU if available\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #'cpu' # 'cuda' or 'cpu'\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# transforms functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "import numbers\n",
        "import collections\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image, ImageOps\n",
        "try:\n",
        "    import accimage\n",
        "except ImportError:\n",
        "    accimage = None\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    \"\"\"Composes several transforms together.\n",
        "    Args:\n",
        "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
        "    Example:\n",
        "        >>> transforms.Compose([\n",
        "        >>>     transforms.CenterCrop(10),\n",
        "        >>>     transforms.ToTensor(),\n",
        "        >>> ])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img, inv=False, flow=False):\n",
        "        for t in self.transforms:\n",
        "            img = t(img, inv, flow)\n",
        "        return img\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        for t in self.transforms:\n",
        "            t.randomize_parameters()\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n",
        "    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n",
        "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, norm_value=255):\n",
        "        self.norm_value = norm_value\n",
        "\n",
        "    def __call__(self, pic, inv, flow):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n",
        "        Returns:\n",
        "            Tensor: Converted image.\n",
        "        \"\"\"\n",
        "        if isinstance(pic, np.ndarray):\n",
        "            # handle numpy array\n",
        "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
        "            # backward compatibility\n",
        "            return img.float().div(self.norm_value)\n",
        "\n",
        "        if accimage is not None and isinstance(pic, accimage.Image):\n",
        "            nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n",
        "            pic.copyto(nppic)\n",
        "            return torch.from_numpy(nppic)\n",
        "\n",
        "        # handle PIL Image\n",
        "        if pic.mode == 'I':\n",
        "            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
        "        elif pic.mode == 'I;16':\n",
        "            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
        "        else:\n",
        "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
        "        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
        "        if pic.mode == 'YCbCr':\n",
        "            nchannel = 3\n",
        "        elif pic.mode == 'I;16':\n",
        "            nchannel = 1\n",
        "        else:\n",
        "            nchannel = len(pic.mode)\n",
        "        img = img.view(pic.size[1], pic.size[0], nchannel)\n",
        "        # put it from HWC to CHW format\n",
        "        # yikes, this transpose takes 80% of the loading time/CPU\n",
        "        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
        "        if isinstance(img, torch.ByteTensor):\n",
        "            return img.float().div(self.norm_value)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "    \"\"\"Normalize an tensor image with mean and standard deviation.\n",
        "    Given mean: (R, G, B) and std: (R, G, B),\n",
        "    will normalize each channel of the torch.*Tensor, i.e.\n",
        "    channel = (channel - mean) / std\n",
        "    Args:\n",
        "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
        "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
        "            respecitvely.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor, inv, flow):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        # TODO: make efficient\n",
        "        if flow is True:\n",
        "            mean = [np.mean(self.mean)]\n",
        "            std = [np.mean(self.std)]\n",
        "        else:\n",
        "            mean = self.mean\n",
        "            std = self.std\n",
        "        for t, m, s in zip(tensor, mean, std):\n",
        "            t.sub_(m).div_(s)\n",
        "        return tensor\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class Scale(object):\n",
        "    \"\"\"Rescale the input PIL.Image to the given size.\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size. If size is a sequence like\n",
        "            (w, h), output size will be matched to this. If size is an int,\n",
        "            smaller edge of the image will be matched to this number.\n",
        "            i.e, if height > width, then image will be rescaled to\n",
        "            (size * height / width, size)\n",
        "        interpolation (int, optional): Desired interpolation. Default is\n",
        "            ``PIL.Image.BILINEAR``\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, img, inv, flow):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL.Image): Image to be scaled.\n",
        "        Returns:\n",
        "            PIL.Image: Rescaled image.\n",
        "        \"\"\"\n",
        "        if isinstance(self.size, int):\n",
        "            w, h = img.size\n",
        "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
        "                return img\n",
        "            if w < h:\n",
        "                ow = self.size\n",
        "                oh = int(self.size * h / w)\n",
        "                return img.resize((ow, oh), self.interpolation)\n",
        "            else:\n",
        "                oh = self.size\n",
        "                ow = int(self.size * w / h)\n",
        "                return img.resize((ow, oh), self.interpolation)\n",
        "        else:\n",
        "            return img.resize(self.size, self.interpolation)\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    \"\"\"Crops the given PIL.Image at the center.\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w), a square crop (size, size) is\n",
        "            made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    def __call__(self, img, inv, flow):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL.Image): Image to be cropped.\n",
        "        Returns:\n",
        "            PIL.Image: Cropped image.\n",
        "        \"\"\"\n",
        "        w, h = img.size\n",
        "        th, tw = self.size\n",
        "        x1 = int(round((w - tw) / 2.))\n",
        "        y1 = int(round((h - th) / 2.))\n",
        "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    \"\"\"Horizontally flip the given PIL.Image randomly with a probability of 0.5.\"\"\"\n",
        "\n",
        "    def __call__(self, img, inv, flow):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL.Image): Image to be flipped.\n",
        "        Returns:\n",
        "            PIL.Image: Randomly flipped image.\n",
        "        \"\"\"\n",
        "        if self.p < 0.5:\n",
        "            img =  img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            if inv is True:\n",
        "                img = ImageOps.invert(img)\n",
        "        return img\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        self.p = random.random()\n",
        "\n",
        "\n",
        "class MultiScaleCornerCrop(object):\n",
        "    \"\"\"Crop the given PIL.Image to randomly selected size.\n",
        "    A crop of size is selected from scales of the original size.\n",
        "    A position of cropping is randomly selected from 4 corners and 1 center.\n",
        "    This crop is finally resized to given size.\n",
        "    Args:\n",
        "        scales: cropping scales of the original size\n",
        "        size: size of the smaller edge\n",
        "        interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scales, size, interpolation=Image.BILINEAR):\n",
        "        self.scales = scales\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "        self.crop_positions = ['c', 'tl', 'tr', 'bl', 'br']\n",
        "\n",
        "    def __call__(self, img, inv, flow):\n",
        "        # print(img.size[0])\n",
        "        min_length = min(img.size[0], img.size[1])\n",
        "        crop_size = int(min_length * self.scale)\n",
        "\n",
        "        image_width = img.size[0]\n",
        "        image_height = img.size[1]\n",
        "\n",
        "        if self.crop_position == 'c':\n",
        "            center_x = image_width // 2\n",
        "            center_y = image_height // 2\n",
        "            box_half = crop_size // 2\n",
        "            x1 = center_x - box_half\n",
        "            y1 = center_y - box_half\n",
        "            x2 = center_x + box_half\n",
        "            y2 = center_y + box_half\n",
        "        elif self.crop_position == 'tl':\n",
        "            x1 = 0\n",
        "            y1 = 0\n",
        "            x2 = crop_size\n",
        "            y2 = crop_size\n",
        "        elif self.crop_position == 'tr':\n",
        "            x1 = image_width - crop_size\n",
        "            y1 = 1\n",
        "            x2 = image_width\n",
        "            y2 = crop_size\n",
        "        elif self.crop_position == 'bl':\n",
        "            x1 = 1\n",
        "            y1 = image_height - crop_size\n",
        "            x2 = crop_size\n",
        "            y2 = image_height\n",
        "        elif self.crop_position == 'br':\n",
        "            x1 = image_width - crop_size\n",
        "            y1 = image_height - crop_size\n",
        "            x2 = image_width\n",
        "            y2 = image_height\n",
        "\n",
        "        img = img.crop((x1, y1, x2, y2))\n",
        "\n",
        "        return img.resize((self.size, self.size), self.interpolation)\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
        "        self.crop_position = self.crop_positions[random.randint(0, len(self.crop_positions) - 1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FiveCrops(object):\n",
        "    \"\"\"Crop the given PIL.Image to randomly selected size.\n",
        "    A crop of size is selected from scales of the original size.\n",
        "    A position of cropping is randomly selected from 4 corners and 1 center.\n",
        "    This crop is finally resized to given size.\n",
        "    Args:\n",
        "        scales: cropping scales of the original size\n",
        "        size: size of the smaller edge\n",
        "        interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], interpolation=Image.BILINEAR, tenCrops=False):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.to_Tensor = ToTensor()\n",
        "        self.normalize = Normalize(self.mean, self.std)\n",
        "        self.tenCrops = tenCrops\n",
        "\n",
        "    def __call__(self, img, inv, flow):\n",
        "        # print(img.size[0])\n",
        "        crop_size = self.size\n",
        "\n",
        "        image_width = img.size[0]\n",
        "        image_height = img.size[1]\n",
        "        crop_positions = []\n",
        "        # center\n",
        "        center_x = image_width // 2\n",
        "        center_y = image_height // 2\n",
        "        box_half = crop_size // 2\n",
        "        x1 = center_x - box_half\n",
        "        y1 = center_y - box_half\n",
        "        x2 = center_x + box_half\n",
        "        y2 = center_y + box_half\n",
        "        crop_positions += [[x1, y1, x2, y2]]\n",
        "    # tl\n",
        "        x1 = 0\n",
        "        y1 = 0\n",
        "        x2 = crop_size\n",
        "        y2 = crop_size\n",
        "        crop_positions += [[x1, y1, x2, y2]]\n",
        "        # tr\n",
        "        x1 = image_width - crop_size\n",
        "        y1 = 1\n",
        "        x2 = image_width\n",
        "        y2 = crop_size\n",
        "        crop_positions += [[x1, y1, x2, y2]]\n",
        "        # bl\n",
        "        x1 = 1\n",
        "        y1 = image_height - crop_size\n",
        "        x2 = crop_size\n",
        "        y2 = image_height\n",
        "        crop_positions += [[x1, y1, x2, y2]]\n",
        "        # br\n",
        "        x1 = image_width - crop_size\n",
        "        y1 = image_height - crop_size\n",
        "        x2 = image_width\n",
        "        y2 = image_height\n",
        "        crop_positions += [[x1, y1, x2, y2]]\n",
        "        cropped_imgs = [img.crop(crop_positions[i]).resize((self.size, self.size), self.interpolation) for i in range(5)]\n",
        "        # cropped_imgs = [img.resize(self.size, self.size, self.interpolation) for img in cropped_imgs]\n",
        "        if self.tenCrops is True:\n",
        "            if inv is True:\n",
        "                flipped_imgs = [ImageOps.invert(cropped_imgs[i].transpose(Image.FLIP_LEFT_RIGHT)) for i in range(5)]\n",
        "            else:\n",
        "                flipped_imgs = [cropped_imgs[i].transpose(Image.FLIP_LEFT_RIGHT) for i in range(5)]\n",
        "            cropped_imgs += flipped_imgs\n",
        "                # cropped_imgs.append(img1.transpose(Image.FLIP_LEFT_RIGHT))\n",
        "\n",
        "        tensor_imgs = [self.to_Tensor(img, inv, flow) for img in cropped_imgs]\n",
        "\n",
        "        normalized_imgs = [self.normalize(img, inv, flow) for img in tensor_imgs]\n",
        "        fiveCropImgs = torch.stack(normalized_imgs, 0)\n",
        "        return fiveCropImgs\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        pass\n",
        "\n",
        "class TenCrops(object):\n",
        "    \"\"\"Crop the given PIL.Image to randomly selected size.\n",
        "    A crop of size is selected from scales of the original size.\n",
        "    A position of cropping is randomly selected from 4 corners and 1 center.\n",
        "    This crop is finally resized to given size.\n",
        "    Args:\n",
        "        scales: cropping scales of the original size\n",
        "        size: size of the smaller edge\n",
        "        interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], interpolation=Image.BILINEAR):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.fiveCrops = FiveCrops(self.size, self.mean, self.std, self.interpolation, True)\n",
        "\n",
        "    def __call__(self, img, inv, flow):\n",
        "        # print(img.size[0])\n",
        "        return self.fiveCrops(img, inv, flow)\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class FlippedImagesTest(object):\n",
        "    \"\"\"Image and its horizontally flipped versions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0]):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.to_Tensor = ToTensor()\n",
        "        self.normalize = Normalize(self.mean, self.std)\n",
        "\n",
        "    def __call__(self, img, inv, flow):\n",
        "        # print(img.size[0])\n",
        "        img_flipped = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        if inv is True:\n",
        "            img_flipped = ImageOps.invert(img_flipped)\n",
        "\n",
        "        # center\n",
        "\n",
        "        tensor_img = self.to_Tensor(img, inv, flow)\n",
        "        tensor_img_flipped = self.to_Tensor(img_flipped, inv, flow)\n",
        "\n",
        "        normalized_img = self.normalize(tensor_img, inv, flow)\n",
        "        normalized_img_flipped = self.normalize(tensor_img_flipped, inv, flow)\n",
        "        horFlippedTest_imgs = [normalized_img, normalized_img_flipped]\n",
        "        horFlippedTest_imgs = torch.stack(horFlippedTest_imgs, 0)\n",
        "        return horFlippedTest_imgs\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        pass\n",
        "    \n",
        "\n",
        "class DownSampling(object):\n",
        "    \n",
        "    \n",
        "    def __init__(self, len_x = 224, len_y = 224, num_x = 7, num_y = 7):\n",
        "        \n",
        "        self.len_x = len_x\n",
        "        self.num_x = num_x\n",
        "        \n",
        "        self.len_y = len_y\n",
        "        self.num_y = num_y\n",
        "\n",
        "    \n",
        "    def __call__(self, tensor, inv, flow):\n",
        "        \n",
        "        tensor = tensor[0]\n",
        "        \n",
        "        pos_x = self.__getPositions(self.len_x, self.num_x)\n",
        "        pos_y = self.__getPositions(self.len_y, self.num_y)\n",
        "        \n",
        "        new_tensor = []\n",
        "\n",
        "        for i, x in enumerate(pos_x):\n",
        "            new_tensor.append([])\n",
        "            for y in pos_y:\n",
        "                new_tensor[i].append(int(tensor[x][y]))\n",
        "        \n",
        "        return torch.Tensor(new_tensor)\n",
        "    \n",
        "    \n",
        "    def __getPositions(self, length, num):\n",
        "    \n",
        "        pos = []\n",
        "\n",
        "        step = int(length/num)\n",
        "        curr_pos = int(np.ceil((length%num)/2))\n",
        "\n",
        "        if curr_pos == 0:\n",
        "            curr_pos = int(length/(2*num))\n",
        "\n",
        "        while curr_pos < length:\n",
        "            pos.append(curr_pos)\n",
        "            curr_pos += step\n",
        "\n",
        "        return pos\n",
        "    \n",
        "    \n",
        "    def randomize_parameters(self):\n",
        "        pass\n",
        "\n",
        "    \n",
        "class KNN_DownSampling(object):\n",
        "    \n",
        "    \n",
        "    def __init__(self, len_x = 224, len_y = 224, num_x = 7, num_y = 7, K = 0, regression = False, full224 = False):\n",
        "        \n",
        "        self.len_x = len_x\n",
        "        self.num_x = num_x\n",
        "        \n",
        "        self.len_y = len_y\n",
        "        self.num_y = num_y\n",
        "        \n",
        "        self.K = K\n",
        "        \n",
        "        self.regression = regression\n",
        "        self.full224 = full224\n",
        "\n",
        "    \n",
        "    def __call__(self, tensor, inv, flow):\n",
        "        \n",
        "        K = self.K\n",
        "        \n",
        "        tensor = tensor[0]\n",
        "        \n",
        "        pos_x = self.__getPositions(self.len_x, self.num_x)\n",
        "        pos_y = self.__getPositions(self.len_y, self.num_y)\n",
        "        \n",
        "        new_tensor = []\n",
        "\n",
        "        if not self.full224:\n",
        "            \n",
        "            for i, x in enumerate(pos_x):\n",
        "\n",
        "                new_tensor.append([])\n",
        "                start_x = x - K\n",
        "                end_x = x + K + 1\n",
        "\n",
        "                if start_x < 0 or end_x > self.len_x:\n",
        "                    raise Exception(\"ERROR - x out of bounds\")\n",
        "\n",
        "                for y in pos_y:\n",
        "\n",
        "                    start_y = y - K\n",
        "                    end_y = y + K + 1\n",
        "\n",
        "                    if start_y < 0 or end_y > self.len_y:\n",
        "                        raise Exception(\"ERROR - y out of bounds\")\n",
        "\n",
        "                    if not self.regression:\n",
        "                        value = round(int(tensor[start_x:end_x, start_y:end_y].sum())/((2*K+1)**2), 0)\n",
        "                    else:\n",
        "                        value = tensor[start_x:end_x, start_y:end_y].sum()/((2*K+1)**2)\n",
        "\n",
        "                    new_tensor[i].append(value)\n",
        "            \n",
        "        else:\n",
        "\n",
        "            step = int(224/self.num_x)\n",
        "            jumps = np.arange(0, 224, step)\n",
        "\n",
        "            for pos, i in enumerate(jumps):\n",
        "                \n",
        "                new_tensor.append([])\n",
        "\n",
        "                for j in jumps:\n",
        "\n",
        "                    if not self.regression:\n",
        "                        value = round(int(tensor[i:i+step, j:j+step].sum())/(step**2), 0)\n",
        "                    else:\n",
        "                        value = tensor[i:i+step, j:j+step].sum()/(step**2)\n",
        "                    \n",
        "                    new_tensor[pos].append(value)\n",
        "                \n",
        "        \n",
        "        return torch.Tensor(new_tensor)\n",
        "    \n",
        "    \n",
        "    def __getPositions(self, length, num):\n",
        "    \n",
        "        pos = []\n",
        "\n",
        "        step = int(length/num)\n",
        "        curr_pos = int(np.ceil((length%num)/2))\n",
        "\n",
        "        if curr_pos == 0:\n",
        "            curr_pos = int(length/(2*num))\n",
        "\n",
        "        while curr_pos < length:\n",
        "            pos.append(curr_pos)\n",
        "            curr_pos += step\n",
        "\n",
        "        return pos\n",
        "    \n",
        "    \n",
        "    def randomize_parameters(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class To1Dimension(object):\n",
        "    \n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "        \n",
        "    def __call__(self, tensor, inv, flow):\n",
        "        \n",
        "        l = []\n",
        "        \n",
        "        for i in tensor:\n",
        "            for j in i:\n",
        "                l.append(int(j))\n",
        "        \n",
        "        return torch.Tensor(l)\n",
        "    \n",
        "    \n",
        "    def randomize_parameters(self):\n",
        "        pass    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dataset definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from random import randrange\n",
        "\n",
        "IMAGE = 0\n",
        "LABEL = 1\n",
        "TEST_USER = 'S2'\n",
        "# directory containing the x-flows frames\n",
        "FLOW_X_FOLDER = \"flow_x_processed\"\n",
        "# directory containing the y-flows frames\n",
        "FLOW_Y_FOLDER = \"flow_y_processed\"\n",
        "# directory containing the rgb frames\n",
        "FRAME_FOLDER = \"processed_frames2\"\n",
        "RGB_FOLDER = 'rgb'\n",
        "RGB_FILENAME = 'rgb'\n",
        "MMAPS_FOLDER = 'mmaps'\n",
        "MMAP_FILENAME = 'map'\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    # functions that loads an image as an rgb pil object\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "\n",
        "def grey_scale_pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    # functions that loads an image as a grey-scale pil object\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('L')\n",
        "\n",
        "\n",
        "class GTEA61(VisionDataset):\n",
        "    # this class inherites from VisionDataset and represents the rgb frames of the dataset\n",
        "    def __init__(self, root, split='train', seq_len=16, transform=None, target_transform=None,\n",
        "                 label_map=None, mmaps=False, mmaps_transform=None, static_frames=False):\n",
        "        super(GTEA61, self).__init__(root, transform=transform, target_transform=target_transform)\n",
        "        self.datadir = root\n",
        "        # split indicates whether we should load the train or test split\n",
        "        self.split = split\n",
        "        self.get_mmaps = mmaps\n",
        "        # seq len tells us how many frames for each video we are going to consider\n",
        "        # frames will be taken uniformly spaced\n",
        "        self.seq_len = seq_len\n",
        "        self.label_map = label_map\n",
        "        self.mmaps = mmaps\n",
        "        self.mmaps_transform = mmaps_transform\n",
        "        self.static_frames = static_frames\n",
        "\n",
        "        if label_map is None:\n",
        "            # if the label map dictionary is not provided, we are going to build it\n",
        "            self.label_map = {}\n",
        "        # videos is a list containing for each video, its path where you can find all its frames\n",
        "        # whereas mmaps contains the path of the mmaps\n",
        "        self.videos = []\n",
        "        if mmaps:\n",
        "            self.mmaps = []\n",
        "        # labels[i] contains the class ID of the i-th video\n",
        "        self.labels = []\n",
        "        # n_frames[i] contains the number of frames available for i-th video\n",
        "        self.n_frames = []\n",
        "        # check if ToTensor is among the transformations\n",
        "        check_totensor = [isinstance(tr, ToTensor) for tr in self.transform.transforms]\n",
        "        self.has_to_tensor = True in check_totensor\n",
        "        if not self.has_to_tensor:\n",
        "            raise ValueError(\"you did NOT provide ToTensor as a transformation for rgbs\")\n",
        "        \n",
        "        if mmaps:\n",
        "            check_mmaps_totensor = [isinstance(tr, ToTensor) for tr in self.mmaps_transform.transforms]\n",
        "            self.mmaps_has_to_tensor = True in check_mmaps_totensor\n",
        "            if not self.mmaps_has_to_tensor:\n",
        "                raise ValueError(\"you did NOT provide ToTensor as a transformation for mmaps\")\n",
        "\n",
        "        # we expect datadir to be GTEA61, so we add FRAME_FOLDER to get to the frames\n",
        "        frame_dir = os.path.join(self.datadir, FRAME_FOLDER)\n",
        "        users = os.listdir(frame_dir)\n",
        "        users_tmp = []\n",
        "        for i in users:\n",
        "            if i != '.DS_Store':\n",
        "                users_tmp.append(i)\n",
        "        users = users_tmp\n",
        "\n",
        "        print(users)\n",
        "        if len(users) < 4:\n",
        "            raise FileNotFoundError(\"you specified the wrong directory\")\n",
        "        if TEST_USER not in users:\n",
        "            raise FileNotFoundError(\"S2 folder not found\")\n",
        "        if self.split == 'test':\n",
        "            folders = [users[users.index(TEST_USER)]]\n",
        "        else:\n",
        "            users.remove(TEST_USER)\n",
        "            folders = users\n",
        "\n",
        "        # folders is a list that contains either :\n",
        "        #   - 1 element -> the path of the folder of the user S2 if split == 'test'\n",
        "        #   - 3 elements -> the paths of the folders for S1,S3,S4 if split == 'train'\n",
        "\n",
        "        if label_map is None:\n",
        "            # now we build the label map; we take folders[0] just to get all class names\n",
        "            # since it is GUARANTEED that all users have same classes\n",
        "            classes = sorted(os.listdir(os.path.join(frame_dir, folders[0])))\n",
        "            classes_tmp = []\n",
        "            for i in classes:\n",
        "                if '.DS_Store' not in i :\n",
        "                    classes_tmp.append(i)\n",
        "            classes = classes_tmp\n",
        "            self.label_map = {act: i for i, act in enumerate(classes)}\n",
        "        for user in sorted(folders):\n",
        "            if \".DS_Store\" in user:\n",
        "                continue\n",
        "            user_dir = os.path.join(frame_dir, user)\n",
        "            # user dir it's gonna be ../GTEA61/processed_frames2/S1 or any other user\n",
        "            for action in sorted(os.listdir(user_dir)):\n",
        "                if \".DS_Store\" in action:\n",
        "                    continue\n",
        "                action_dir = os.path.join(user_dir, action)\n",
        "                # inside an action dir we can have 1 or more videos\n",
        "                for element in sorted(os.listdir(action_dir)):\n",
        "                    if \".DS_Store\" in element:\n",
        "                        continue\n",
        "\n",
        "                    # we add rgb to the path since there is an additional folder inside S1/1/rgb\n",
        "                    # before the frames\n",
        "                    frames = os.path.join(action_dir, element, RGB_FOLDER)\n",
        "                    if self.get_mmaps:\n",
        "                        mmap = os.path.join(action_dir, element, MMAPS_FOLDER)\n",
        "                        self.mmaps.append(mmap)\n",
        "                    # we append in videos the path\n",
        "                    self.videos.append(frames)\n",
        "                    # in labels the label, using the label map\n",
        "                    self.labels.append(self.label_map[action])\n",
        "                    # in frames its length in number of frames\n",
        "                    self.n_frames.append(len(os.listdir(frames)))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # firstly we retrieve the video path, label and num of frames\n",
        "        vid = self.videos[index]\n",
        "        label = self.labels[index]\n",
        "        length = self.n_frames[index]\n",
        "        if self.transform is not None:\n",
        "            # this is needed to randomize the parameters of the random transformations\n",
        "            self.transform.randomize_parameters()\n",
        "            \n",
        "        if self.mmaps:\n",
        "            if self.mmaps_transform is not None:\n",
        "                self.mmaps_transform.randomize_parameters()\n",
        "\n",
        "        # sort the list of frames since the name is like rgb002.png\n",
        "        # so we use the last number as an ordering\n",
        "        frames = np.array(sorted(os.listdir(vid)))\n",
        "        # now we take seq_len equally spaced frames between 0 and length\n",
        "        # linspace with the option int will give us the indices to take\n",
        "        select_indices = np.linspace(0, length, self.seq_len, endpoint=False, dtype=int)\n",
        "        # we then select the frames using numpy fancy indexing\n",
        "        # note that the numpy arrays are arrays of strings, containing the file names\n",
        "        # nevertheless, numpy will work with string arrays as well\n",
        "        select_frames = frames[select_indices]\n",
        "        # append to each file its path\n",
        "        select_files = [os.path.join(vid, frame) for frame in select_frames]\n",
        "        # use pil_loader to get pil objects\n",
        "        sequence = [pil_loader(file) for file in select_files]\n",
        "        if self.get_mmaps:\n",
        "            # replace folder\n",
        "            select_map = [os.path.join(os.path.dirname(file).replace(RGB_FOLDER, MMAPS_FOLDER), os.path.basename(file).replace(RGB_FILENAME, MMAP_FILENAME) ) for file in select_files]\n",
        "            maps_sequence = [grey_scale_pil_loader(file) for file in select_map]\n",
        "    \n",
        "        # Applies preprocessing when accessing the image\n",
        "        \n",
        "        if not self.static_frames:\n",
        "            \n",
        "            if self.transform is not None:\n",
        "                sequence = [self.transform(image) for image in sequence]\n",
        "                # now, if the ToTensor transformation is applied\n",
        "                # we have in sequence a list of tensor, so we use stack along dimension 0\n",
        "                # to create a tensor with one more dimension that contains them all\n",
        "                if self.has_to_tensor:\n",
        "                    sequence = torch.stack(sequence, 0)\n",
        "\n",
        "                if self.get_mmaps:\n",
        "                    maps_sequence = [self.mmaps_transform(mmap) for mmap in maps_sequence]\n",
        "                    if self.has_to_tensor:\n",
        "                        maps_sequence = torch.stack(maps_sequence, 0)\n",
        "                    maps_sequence = maps_sequence.squeeze(1)\n",
        "\n",
        "                    return sequence, maps_sequence, label\n",
        "\n",
        "            return sequence, label\n",
        "        \n",
        "        else:\n",
        "            \n",
        "            random_number = randrange(self.seq_len)\n",
        "            \n",
        "            if self.transform is not None:\n",
        "                sequence = [self.transform(image) for image in sequence]\n",
        "                # now, if the ToTensor transformation is applied\n",
        "                # we have in sequence a list of tensor, so we use stack along dimension 0\n",
        "                # to create a tensor with one more dimension that contains them all\n",
        "                if self.has_to_tensor:\n",
        "                    sequence = torch.stack(sequence, 0)\n",
        "                \n",
        "                static_sequence = [sequence[random_number] for i in range(self.seq_len)]\n",
        "                \n",
        "                if self.has_to_tensor:\n",
        "                    static_sequence = torch.stack(static_sequence, 0)\n",
        "                \n",
        "                if self.get_mmaps:\n",
        "                    maps_sequence = [self.mmaps_transform(mmap) for mmap in maps_sequence]\n",
        "                    if self.has_to_tensor:\n",
        "                        maps_sequence = torch.stack(maps_sequence, 0)\n",
        "                    maps_sequence = maps_sequence.squeeze(1)\n",
        "\n",
        "                    return sequence, static_sequence, maps_sequence, label\n",
        "\n",
        "            return sequence, static_sequence, label\n",
        "            \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos)\n",
        "\n",
        "\n",
        "class GTEA61_flow(VisionDataset):\n",
        "    # this class inherites from VisionDataset and represents the rgb frames of the dataset\n",
        "    def __init__(self, root, split='train', seq_len=5, transform=None, target_transform=None,\n",
        "                 label_map=None, n_seq=-1):\n",
        "        super(GTEA61_flow, self).__init__(root, transform=transform, target_transform=target_transform)\n",
        "        # we expect datadir to be ../GTEA61\n",
        "        self.datadir = root\n",
        "        # split indicates whether we should load the train or test split\n",
        "        self.split = split\n",
        "        self.n_seq = n_seq\n",
        "        # seq len here tells us how many optical frames for each video\n",
        "        # we are going to consider; note that now\n",
        "        # frames will be sequential and not uniformly spaced\n",
        "        self.seq_len = seq_len\n",
        "        self.label_map = label_map\n",
        "        if label_map is None:\n",
        "            # if the label map dictionary is not provided, we are going to build it\n",
        "            self.label_map = {}\n",
        "        # x_frames is a list containing for each flow video, its path, where you can find all its frames\n",
        "        # it will contain the ones under flow_x_processed\n",
        "        self.x_frames = []\n",
        "        # y_frames is the same as x_frames, but contains the ones under flow_y_processed\n",
        "        self.y_frames = []\n",
        "        # labels[i] contains the class ID of the i-th video\n",
        "        self.labels = []\n",
        "        # n_frames[i] contains the number of frames available for i-th video\n",
        "        self.n_frames = []\n",
        "        # check if ToTensor is among the transformations\n",
        "        check_totensor = [isinstance(tr, ToTensor) for tr in self.transform.transforms]\n",
        "        self.has_to_tensor = True in check_totensor\n",
        "        if not self.has_to_tensor:\n",
        "            raise ValueError(\"you did NOT provide ToTensor as a transformation\")\n",
        "\n",
        "        # we expect datadir to be GTEA61, so we add the flow folder to get to the flow frames\n",
        "        flow_dir = os.path.join(self.datadir, FLOW_X_FOLDER)\n",
        "        users = os.listdir(flow_dir)\n",
        "        if len(users) != 4:\n",
        "            raise FileNotFoundError(\"you specified the wrong directory\")\n",
        "        if TEST_USER not in users:\n",
        "            raise FileNotFoundError(\"S2 folder not found\")\n",
        "        if self.split == 'test':\n",
        "            folders = [users[users.index(TEST_USER)]]\n",
        "        else:\n",
        "            users.remove(TEST_USER)\n",
        "            folders = users\n",
        "\n",
        "        # folders is a list that contains either :\n",
        "        #   - 1 element -> the path of the folder of the user S2 if split == 'test'\n",
        "        #   - 3 elements -> the paths of the folders for S1,S3,S4 if split == 'train'\n",
        "\n",
        "        if label_map is None:\n",
        "            # now we build the label map; we take folders[0] just to get all class names\n",
        "            # since it is GUARANTEED that all users have same classes\n",
        "            classes = os.listdir(os.path.join(flow_dir, folders[0]))\n",
        "            self.label_map = {act: i for i, act in enumerate(classes)}\n",
        "\n",
        "        for user in folders:\n",
        "            # user dir it's gonna be ../GTEA61/flow_x_processed/S1 or any other user\n",
        "            user_dir = os.path.join(flow_dir, user)\n",
        "            for action in os.listdir(user_dir):\n",
        "                # inside an action dir we can have 1 or more videos\n",
        "                action_dir = os.path.join(user_dir, action)\n",
        "                for element in os.listdir(action_dir):\n",
        "                    frames = os.path.join(action_dir, element)\n",
        "                    # we put in x_frames the path to the folder with all the flow frames\n",
        "                    self.x_frames.append(frames)\n",
        "                    # the path for the y_frames is the same as x, except that we replace\n",
        "                    # flow_x_processed with flow_y_processed in the path\n",
        "                    # it is GUARANTEED that for each action we have the same number\n",
        "                    # of x and y frames\n",
        "                    self.y_frames.append(frames.replace(FLOW_X_FOLDER, FLOW_Y_FOLDER))\n",
        "                    # put the label in label using the label map dictionary\n",
        "                    self.labels.append(self.label_map[action])\n",
        "                    # put here the number of flow frames\n",
        "                    self.n_frames.append(len(os.listdir(frames)))\n",
        "\n",
        "    def get_selected_files(self, vid_x, frames_x, frames_y, select_indices):\n",
        "        # select the frames using numpy fancy indexing\n",
        "        # note these are arrays of strings, containing the file names\n",
        "        select_x_frames = frames_x[select_indices]\n",
        "        select_y_frames = frames_y[select_indices]\n",
        "        # this will position the elements of select_x_frames and select_y_frames\n",
        "        # alternatively in a numpy array. remember these file names of the frames\n",
        "        select_frames = np.ravel(np.column_stack((select_x_frames, select_y_frames)))\n",
        "        # append to each file the root path. we use the one for  x frames,\n",
        "        # then replace with y for y frames.x frames are in even positions, y in odd positions\n",
        "        select_files = [os.path.join(vid_x, frame) for frame in select_frames]\n",
        "        select_files[1::2] = [y_files.replace('x', 'y') for y_files in select_files[1::2]]\n",
        "        # create pil objects\n",
        "        sequence = [grey_scale_pil_loader(file) for file in select_files]\n",
        "        # Applies preprocessing when accessing the image\n",
        "        if self.transform is not None:\n",
        "            # inv=True will create the negative image for x frames\n",
        "            sequence[::2] = [self.transform(image, inv=True, flow=True) for image in sequence[::2]]\n",
        "            sequence[1::2] = [self.transform(image, inv=False, flow=True) for image in sequence[1::2]]\n",
        "            # if the ToTensor transformation is applied\n",
        "            # 'sequence' is a list of tensors, so we stack along dimension 0 in a single tensor\n",
        "            # then we apply squeeze along the 1 dimension, because the images are grey-scale,\n",
        "            # so there is only one channel and we eliminate that dimension\n",
        "            if self.has_to_tensor:\n",
        "                sequence = torch.stack(sequence, 0).squeeze(1)\n",
        "        return sequence\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get the paths of the x video, y, label and length\n",
        "        vid_x = self.x_frames[index]\n",
        "        vid_y = self.y_frames[index]\n",
        "        label = self.labels[index]\n",
        "        length = self.n_frames[index]\n",
        "        # needed to randomize the parameters of the custom transformations\n",
        "        self.transform.randomize_parameters()\n",
        "        # sort list of frames since the name is like flow_x_002.png, last number as ordering\n",
        "        frames_x = np.array(sorted(os.listdir(vid_x)))\n",
        "        # do the same for y\n",
        "        frames_y = np.array(sorted(os.listdir(vid_y)))\n",
        "        if self.n_seq > 0:\n",
        "            segments = []\n",
        "            starting_frames = np.linspace(1, length-self.seq_len+1, self.n_seq, endpoint=False, dtype=int)\n",
        "            for start_frame in starting_frames:\n",
        "                select_indices = start_frame + np.arange(0, self.seq_len)\n",
        "                sequence = self.get_selected_files(vid_x, frames_x, frames_y, select_indices)\n",
        "                segments.append(sequence)\n",
        "            segments = torch.stack(segments, 0)\n",
        "\n",
        "            return segments, label\n",
        "        else:\n",
        "            if self.split == 'train':\n",
        "                # if we are training, we take a random starting frame\n",
        "                start_frame = random.randint(0, length - self.seq_len)\n",
        "            else:\n",
        "                # if we are testing, we take a centered interval\n",
        "                start_frame = np.ceil((length - self.seq_len) / 2).astype('int')\n",
        "            # the frames will be sequential, so the select indices are\n",
        "            # from startFrame to starFrame + seq_len\n",
        "            select_indices = start_frame + np.arange(0, self.seq_len)\n",
        "            sequence = self.get_selected_files(vid_x, frames_x, frames_y, select_indices)\n",
        "\n",
        "            return sequence, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_frames)\n",
        "\n",
        "\n",
        "class GTEA61_2Stream(VisionDataset):\n",
        "    # this class inherites from VisionDataset and represents both rgb and flow frames of the dataset\n",
        "    # it does so by wrapping together an instance of GTEA61 for the rgb frames\n",
        "    # and an instance of GTEA61_flow for the flow frames\n",
        "    def __init__(self, root, split='train', seq_len=7, stack_size=5, transform=None, target_transform=None):\n",
        "        super(GTEA61_2Stream, self).__init__(root, transform=transform, target_transform=target_transform)\n",
        "        # we expect datadir to be ../GTEA61\n",
        "        self.datadir = root\n",
        "        # split indicates whether we should load the train or test split\n",
        "        self.split = split\n",
        "        # seq len is the number of rgb frames. they will be uniformly spaced\n",
        "        self.seq_len = seq_len\n",
        "        # stack size is the number of flow frames. they will be sequential\n",
        "        self.stack_size = stack_size\n",
        "\n",
        "        # now we check that we are in the right directory\n",
        "        frame_dir = os.path.join(self.datadir, FRAME_FOLDER)\n",
        "        users = os.listdir(frame_dir)\n",
        "        if len(users) != 4:\n",
        "            raise FileNotFoundError(\"you specified the wrong directory\")\n",
        "        if TEST_USER not in users:\n",
        "            raise FileNotFoundError(\"S2 folder not found\")\n",
        "        if self.split == 'test':\n",
        "            folders = [users[users.index(TEST_USER)]]\n",
        "        else:\n",
        "            users.remove(TEST_USER)\n",
        "            folders = users\n",
        "        # now we build a label map dictionary and we pass it to the instances of GTEA and GTEA_flow\n",
        "        classes = os.listdir(os.path.join(frame_dir, folders[0]))\n",
        "        self.label_map = {act: i for i, act in enumerate(classes)}\n",
        "        # instance the rgb dataset\n",
        "        self.frame_dataset = GTEA61(self.datadir, split=self.split, seq_len=self.seq_len,\n",
        "                                    transform=self.transform, label_map=self.label_map)\n",
        "        # instance the flow dataset\n",
        "        self.flow_dataset = GTEA61_flow(self.datadir, split=self.split, seq_len=self.stack_size,\n",
        "                                        transform=self.transform, label_map=self.label_map)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # to retrieve an item, we just ask the instances of\n",
        "        # rgb and flow dataset to do it\n",
        "        # then we return both the tensors, and the label\n",
        "        frame_seq, label = self.frame_dataset.__getitem__(index)\n",
        "        flow_seq, _ = self.flow_dataset.__getitem__(index)\n",
        "        return flow_seq, frame_seq, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.frame_dataset.__len__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "wl6fSd3MXofW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.backends import cudnn\n",
        "import torchvision\n",
        "from colorama import init\n",
        "from colorama import Fore, Back, Style\n",
        "\n",
        "from torchvision.models import resnet34\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import sys\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1GznJhObXPk"
      },
      "source": [
        "#**Learning without Temporal information** (avgpool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy4KrHClbAmC"
      },
      "source": [
        "#MAIN PARAMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "w-tz9mHPbCYW",
        "outputId": "5a911630-35ef-41a1-8d9d-d7600842d60d"
      },
      "outputs": [],
      "source": [
        "homework_step = 0 #--> Learning without Temporal information (avgpool)\n",
        "homework_step = 1 #--> Learning with Temporal information (LSTM)\n",
        "homework_step = 2 #--> Learning with Spatio-Temporal information (ConvLSTM)\n",
        "\n",
        "\n",
        "\n",
        "DATA_DIR = 'GTEA61/' #path dataset\n",
        "model_folder = '/content/saved_models/' + \"/\" + \"homework_step\"+ str(homework_step) + \"/\" #path to save model\n",
        "if not os.path.isdir(model_folder):\n",
        "    os.makedirs(model_folder)\n",
        "\n",
        "\n",
        "# All this param can be change!\n",
        "\n",
        "NUM_CLASSES = 61\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.001            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
        "NUM_EPOCHS = 200     # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [25, 75, 150] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "MEM_SIZE = 512       # Dim of internal state of LSTM or ConvLSTM\n",
        "SEQ_LEN = 3          # Num Frames\n",
        "\n",
        "# this dictionary is needed for the logger class\n",
        "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_CLASSES, 'BATCH_SIZE':BATCH_SIZE,\n",
        "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
        "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':MEM_SIZE, 'SEQ_LEN':SEQ_LEN}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPwkOR8taVdN"
      },
      "source": [
        "#Dataloaders & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "LT_Gy79SgBLq"
      },
      "outputs": [],
      "source": [
        "# Normalize\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
        "                             ToTensor(), normalize])\n",
        "spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T69vfGGhjKa_",
        "outputId": "66aadc83-8ec4-4f9b-e882-e4c3af32af50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['S1', 'S2', 'S3', 'S4']\n",
            "['S1', 'S2', 'S3', 'S4']\n",
            "Train Dataset: 341\n",
            "Test Dataset: 116\n"
          ]
        }
      ],
      "source": [
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform, seq_len=SEQ_LEN)\n",
        "test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=SEQ_LEN)\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))\n",
        "\n",
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21wjiwvW-OPQ"
      },
      "source": [
        "#Models - different resnet implementations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, noBN=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.noBN = noBN\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        # print('noBN in basicBlock = ', self.noBN)\n",
        "        outBN = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        outBN = outBN + residual\n",
        "        outBN = self.relu(outBN)\n",
        "        if self.noBN is False:\n",
        "            return outBN\n",
        "        else:\n",
        "            out = out + residual\n",
        "            return outBN, out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, noBN=False):\n",
        "        self.inplanes = 64\n",
        "        self.noBN = noBN\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, noBN=self.noBN)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, noBN=False):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        # print('blocks = ', blocks)\n",
        "        if noBN is False:\n",
        "            # print('with BN')\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "        else:\n",
        "            # print('no BN')\n",
        "            if blocks > 2:\n",
        "                # print('blocks > 2')\n",
        "                for i in range(1, blocks-1):\n",
        "                    layers.append(block(self.inplanes, planes))\n",
        "                layers.append(block(self.inplanes, planes, noBN=True))\n",
        "            else:\n",
        "                # print('blocks <= 2')\n",
        "                layers.append(block(self.inplanes, planes, noBN=True))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        if self.noBN:\n",
        "            conv_layer4BN, conv_layer4NBN = self.layer4(x)\n",
        "        else:\n",
        "            conv_layer4BN = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(conv_layer4BN)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        if self.noBN:\n",
        "            return x, conv_layer4BN, conv_layer4NBN\n",
        "        else:\n",
        "            return x, conv_layer4BN\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, noBN=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], noBN=noBN, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, noBN=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], noBN=noBN, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']), strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#ours model implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "XMWuE-4SHxoY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "# LSTM\n",
        "class MyLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.lin_i_xx = nn.Linear(input_size, hidden_size)\n",
        "        self.lin_i_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        self.lin_f_xx = nn.Linear(input_size, hidden_size)\n",
        "        self.lin_f_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        self.lin_c_xx = nn.Linear(input_size, hidden_size)\n",
        "        self.lin_c_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        self.lin_o_xx = nn.Linear(input_size, hidden_size)\n",
        "        self.lin_o_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        if state is None:\n",
        "            state = (Variable(torch.randn(x.size(0), x.size(1)).cuda()),\n",
        "                     Variable(torch.randn(x.size(0), x.size(1)).cuda()))\n",
        "\n",
        "        ##################################\n",
        "        # You should implement this part #\n",
        "            \n",
        "        ht_1, ct_1 = state\n",
        "        it = torch.sigmoid(self.lin_i_xx(x) + self.lin_i_hh(ht_1))\n",
        "        ft = torch.sigmoid(self.lin_f_xx(x) + self.lin_f_hh(ht_1))\n",
        "        ct_tilde = torch.tanh(self.lin_c_xx(x) + self.lin_c_hh(ht_1))\n",
        "        ct = (ct_tilde * it) + (ct_1 * ft)\n",
        "        ot = torch.sigmoid(self.lin_o_xx(x) + self.lin_o_hh(ht_1))\n",
        "        ht = ot * torch.tanh(ct)\n",
        "        return ht, ct\n",
        "\n",
        "        ##################################\n",
        "\n",
        "        return  None, None\n",
        "\n",
        "\n",
        "#ConvLSTM\n",
        "class MyConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, kernel_size=3, stride=1, padding=1):\n",
        "        super(MyConvLSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.conv_i_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_i_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        self.conv_f_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_f_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        self.conv_c_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_c_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        self.conv_o_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_o_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_i_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_i_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_i_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_f_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_f_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_f_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_c_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_c_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_c_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_o_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_o_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_o_hh.weight)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        if state is None:\n",
        "            state = (Variable(torch.randn(x.size(0), x.size(1), x.size(2), x.size(3)).cuda()),\n",
        "                     Variable(torch.randn(x.size(0), x.size(1), x.size(2), x.size(3)).cuda()))\n",
        "\n",
        "        ##################################\n",
        "        # You should implement this part #\n",
        "            \n",
        "        ht_1, ct_1 = state\n",
        "        it = torch.sigmoid(self.conv_i_xx(x) + self.conv_i_hh(ht_1))\n",
        "        ft = torch.sigmoid(self.conv_f_xx(x) + self.conv_f_hh(ht_1))\n",
        "        ct_tilde = torch.tanh(self.conv_c_xx(x) + self.conv_c_hh(ht_1))\n",
        "        ct = (ct_tilde * it) + (ct_1 * ft)\n",
        "        ot = torch.sigmoid(self.conv_o_xx(x) + self.conv_o_hh(ht_1))\n",
        "        ht = ot * torch.tanh(ct)\n",
        "        return ht, ct\n",
        "        ##################################\n",
        "\n",
        "        return  None, None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Network\n",
        "class ourModel(nn.Module):\n",
        "    def __init__(self, num_classes=61, mem_size=512, homework_step = 0 , DEVICE=\"\"):\n",
        "        super(ourModel, self).__init__()\n",
        "        self.DEVICE = DEVICE\n",
        "        self.num_classes = num_classes\n",
        "        self.resNet = resnet34(True, True)\n",
        "        self.mem_size = mem_size\n",
        "        self.weight_softmax = self.resNet.fc.weight\n",
        "        self.homework_step = homework_step\n",
        "        if self.homework_step == 1:\n",
        "          self.lstm_cell = MyLSTMCell(512, mem_size)\n",
        "        elif self.homework_step == 2:\n",
        "          self.lstm_cell = MyConvLSTMCell(512, mem_size)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "        self.fc = nn.Linear(mem_size, self.num_classes)\n",
        "        self.classifier = nn.Sequential(self.dropout, self.fc)\n",
        "\n",
        "    def forward(self, inputVariable):\n",
        "        #Learning without Temporal information (mean)\n",
        "        if self.homework_step == 0:\n",
        "            video_level_features = torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE)\n",
        "            for t in range(inputVariable.size(0)):\n",
        "                #spatial_frame_feat: (bs, 512, 7, 7)\n",
        "                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n",
        "                #frames_feat: (bs, 512)\n",
        "                frame_feat = self.avgpool(spatial_frame_feat).view(spatial_frame_feat.size(0), -1)\n",
        "                video_level_features = video_level_features + frame_feat\n",
        "\n",
        "            video_level_features = video_level_features / inputVariable.size(0)\n",
        "            logits = self.classifier(video_level_features)\n",
        "            return logits, video_level_features\n",
        "\n",
        "        #Learning with Temporal information (LSTM)\n",
        "        elif self.homework_step == 1:\n",
        "            state = ( torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE),\n",
        "                     torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE) )\n",
        "            for t in range(inputVariable.size(0)):\n",
        "                #spatial_frame_feat: (bs, 512, 7, 7)\n",
        "                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n",
        "                #frames_feat: (bs, 512)\n",
        "                frame_feat = self.avgpool(spatial_frame_feat).view(state[1].size(0), -1)\n",
        "                state = self.lstm_cell(frame_feat, state)\n",
        "\n",
        "            video_level_features = state[1]\n",
        "            logits = self.classifier(video_level_features)\n",
        "            return logits, video_level_features\n",
        "\n",
        "        #Learning with Temporal information (ConvLSTM)\n",
        "        elif self.homework_step == 2:\n",
        "            state = (torch.zeros((inputVariable.size(1), self.mem_size, 7, 7)).to(self.DEVICE),\n",
        "                     torch.zeros((inputVariable.size(1), self.mem_size, 7, 7)).to(self.DEVICE))\n",
        "            for t in range(inputVariable.size(0)):\n",
        "                #spatial_frame_feat: (bs, 512, 7, 7)\n",
        "                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n",
        "                state = self.lstm_cell(spatial_frame_feat, state)\n",
        "            video_level_features = self.avgpool(state[1]).view(state[1].size(0), -1)\n",
        "            logits = self.classifier(video_level_features)\n",
        "            return logits, video_level_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru8vllrMbgvL"
      },
      "source": [
        "#Build Model - Loss - Opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "XZe-ZbEL7z3x"
      },
      "outputs": [],
      "source": [
        "#CUDA_LAUNCH_BLOCKING=1\n",
        "validate = True\n",
        "\n",
        "model = ourModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE, homework_step=homework_step, DEVICE=DEVICE) #model\n",
        "\n",
        "#Train only the lstm cell and classifier\n",
        "model.train(False)\n",
        "for params in model.parameters():\n",
        "    params.requires_grad = False\n",
        "\n",
        "if homework_step > 0:\n",
        "    for params in model.lstm_cell.parameters():\n",
        "        params.requires_grad = True\n",
        "    model.lstm_cell.train(True)\n",
        "\n",
        "for params in model.classifier.parameters():\n",
        "    params.requires_grad = True\n",
        "model.classifier.train(True)\n",
        "\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model_state_dict_rgb_split2.pth\", map_location=torch.device('cpu')), strict=True)\n",
        "\n",
        "\n",
        "#Loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "#Opt\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "#Scheduler\n",
        "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0MWgLingzhw"
      },
      "source": [
        "#Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "p-uE2A9eHmtn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[30mTrain: Epoch = 1 | Loss = 4.551 | Accuracy = 12.500\n",
            "\u001b[32mVal: Epoch = 1 | Loss 2.741 | Accuracy = 26.724\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 2 | Loss = 3.350 | Accuracy = 16.875\n",
            "\u001b[32mVal: Epoch = 2 | Loss 2.647 | Accuracy = 25.000\n",
            "\u001b[30mTrain: Epoch = 3 | Loss = 3.084 | Accuracy = 17.500\n",
            "\u001b[32mVal: Epoch = 3 | Loss 2.466 | Accuracy = 28.448\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 4 | Loss = 2.902 | Accuracy = 21.562\n",
            "\u001b[32mVal: Epoch = 4 | Loss 2.557 | Accuracy = 30.172\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 5 | Loss = 2.809 | Accuracy = 22.188\n",
            "\u001b[32mVal: Epoch = 5 | Loss 2.302 | Accuracy = 36.207\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 6 | Loss = 2.734 | Accuracy = 25.000\n",
            "\u001b[32mVal: Epoch = 6 | Loss 2.409 | Accuracy = 33.621\n",
            "\u001b[30mTrain: Epoch = 7 | Loss = 2.560 | Accuracy = 30.000\n",
            "\u001b[32mVal: Epoch = 7 | Loss 2.273 | Accuracy = 37.931\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 8 | Loss = 2.473 | Accuracy = 30.938\n",
            "\u001b[32mVal: Epoch = 8 | Loss 2.244 | Accuracy = 38.793\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 9 | Loss = 2.534 | Accuracy = 28.438\n",
            "\u001b[32mVal: Epoch = 9 | Loss 2.195 | Accuracy = 39.655\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 10 | Loss = 2.377 | Accuracy = 30.938\n",
            "\u001b[32mVal: Epoch = 10 | Loss 2.178 | Accuracy = 38.793\n",
            "\u001b[30mTrain: Epoch = 11 | Loss = 2.411 | Accuracy = 33.750\n",
            "\u001b[32mVal: Epoch = 11 | Loss 2.209 | Accuracy = 37.931\n",
            "\u001b[30mTrain: Epoch = 12 | Loss = 2.325 | Accuracy = 30.938\n",
            "\u001b[32mVal: Epoch = 12 | Loss 2.182 | Accuracy = 35.345\n",
            "\u001b[30mTrain: Epoch = 13 | Loss = 2.243 | Accuracy = 34.062\n",
            "\u001b[32mVal: Epoch = 13 | Loss 2.157 | Accuracy = 36.207\n",
            "\u001b[30mTrain: Epoch = 14 | Loss = 2.331 | Accuracy = 35.625\n",
            "\u001b[32mVal: Epoch = 14 | Loss 2.128 | Accuracy = 41.379\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 15 | Loss = 2.271 | Accuracy = 35.000\n",
            "\u001b[32mVal: Epoch = 15 | Loss 2.018 | Accuracy = 42.241\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 16 | Loss = 2.199 | Accuracy = 34.688\n",
            "\u001b[32mVal: Epoch = 16 | Loss 2.079 | Accuracy = 42.241\n",
            "\u001b[30mTrain: Epoch = 17 | Loss = 2.175 | Accuracy = 37.812\n",
            "\u001b[32mVal: Epoch = 17 | Loss 2.009 | Accuracy = 44.828\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 18 | Loss = 2.073 | Accuracy = 39.688\n",
            "\u001b[32mVal: Epoch = 18 | Loss 2.033 | Accuracy = 43.966\n",
            "\u001b[30mTrain: Epoch = 19 | Loss = 2.000 | Accuracy = 42.812\n",
            "\u001b[32mVal: Epoch = 19 | Loss 1.914 | Accuracy = 45.690\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 20 | Loss = 2.010 | Accuracy = 41.250\n",
            "\u001b[32mVal: Epoch = 20 | Loss 1.975 | Accuracy = 39.655\n",
            "\u001b[30mTrain: Epoch = 21 | Loss = 2.072 | Accuracy = 40.312\n",
            "\u001b[32mVal: Epoch = 21 | Loss 1.962 | Accuracy = 40.517\n",
            "\u001b[30mTrain: Epoch = 22 | Loss = 2.010 | Accuracy = 40.625\n",
            "\u001b[32mVal: Epoch = 22 | Loss 1.981 | Accuracy = 43.103\n",
            "\u001b[30mTrain: Epoch = 23 | Loss = 2.024 | Accuracy = 40.625\n",
            "\u001b[32mVal: Epoch = 23 | Loss 1.945 | Accuracy = 38.793\n",
            "\u001b[30mTrain: Epoch = 24 | Loss = 2.062 | Accuracy = 39.375\n",
            "\u001b[32mVal: Epoch = 24 | Loss 1.961 | Accuracy = 43.103\n",
            "\u001b[30mTrain: Epoch = 25 | Loss = 1.927 | Accuracy = 41.562\n",
            "\u001b[32mVal: Epoch = 25 | Loss 1.896 | Accuracy = 43.966\n",
            "\u001b[30mTrain: Epoch = 26 | Loss = 1.974 | Accuracy = 41.562\n",
            "\u001b[32mVal: Epoch = 26 | Loss 1.877 | Accuracy = 42.241\n",
            "\u001b[30mTrain: Epoch = 27 | Loss = 1.892 | Accuracy = 45.938\n",
            "\u001b[32mVal: Epoch = 27 | Loss 1.871 | Accuracy = 42.241\n",
            "\u001b[30mTrain: Epoch = 28 | Loss = 1.806 | Accuracy = 50.000\n",
            "\u001b[32mVal: Epoch = 28 | Loss 1.877 | Accuracy = 42.241\n",
            "\u001b[30mTrain: Epoch = 29 | Loss = 1.856 | Accuracy = 47.500\n",
            "\u001b[32mVal: Epoch = 29 | Loss 1.872 | Accuracy = 43.966\n",
            "\u001b[30mTrain: Epoch = 30 | Loss = 1.807 | Accuracy = 45.938\n",
            "\u001b[32mVal: Epoch = 30 | Loss 1.861 | Accuracy = 43.966\n",
            "\u001b[30mTrain: Epoch = 31 | Loss = 1.714 | Accuracy = 48.750\n",
            "\u001b[32mVal: Epoch = 31 | Loss 1.851 | Accuracy = 43.966\n",
            "\u001b[30mTrain: Epoch = 32 | Loss = 1.731 | Accuracy = 50.625\n",
            "\u001b[32mVal: Epoch = 32 | Loss 1.839 | Accuracy = 43.966\n",
            "\u001b[30mTrain: Epoch = 33 | Loss = 1.703 | Accuracy = 47.188\n",
            "\u001b[32mVal: Epoch = 33 | Loss 1.829 | Accuracy = 44.828\n",
            "\u001b[30mTrain: Epoch = 34 | Loss = 1.667 | Accuracy = 50.313\n",
            "\u001b[32mVal: Epoch = 34 | Loss 1.822 | Accuracy = 44.828\n",
            "\u001b[30mTrain: Epoch = 35 | Loss = 1.662 | Accuracy = 51.250\n",
            "\u001b[32mVal: Epoch = 35 | Loss 1.822 | Accuracy = 44.828\n",
            "\u001b[30mTrain: Epoch = 36 | Loss = 1.688 | Accuracy = 49.688\n",
            "\u001b[32mVal: Epoch = 36 | Loss 1.830 | Accuracy = 43.966\n",
            "\u001b[30mTrain: Epoch = 37 | Loss = 1.711 | Accuracy = 52.188\n",
            "\u001b[32mVal: Epoch = 37 | Loss 1.826 | Accuracy = 45.690\n",
            "\u001b[30mTrain: Epoch = 38 | Loss = 1.566 | Accuracy = 52.188\n",
            "\u001b[32mVal: Epoch = 38 | Loss 1.803 | Accuracy = 45.690\n",
            "\u001b[30mTrain: Epoch = 39 | Loss = 1.723 | Accuracy = 49.688\n",
            "\u001b[32mVal: Epoch = 39 | Loss 1.795 | Accuracy = 45.690\n",
            "\u001b[30mTrain: Epoch = 40 | Loss = 1.643 | Accuracy = 52.812\n",
            "\u001b[32mVal: Epoch = 40 | Loss 1.794 | Accuracy = 46.552\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 41 | Loss = 1.482 | Accuracy = 58.750\n",
            "\u001b[32mVal: Epoch = 41 | Loss 1.787 | Accuracy = 46.552\n",
            "\u001b[30mTrain: Epoch = 42 | Loss = 1.640 | Accuracy = 51.562\n",
            "\u001b[32mVal: Epoch = 42 | Loss 1.802 | Accuracy = 45.690\n",
            "\u001b[30mTrain: Epoch = 43 | Loss = 1.671 | Accuracy = 51.875\n",
            "\u001b[32mVal: Epoch = 43 | Loss 1.821 | Accuracy = 46.552\n",
            "\u001b[30mTrain: Epoch = 44 | Loss = 1.657 | Accuracy = 49.062\n",
            "\u001b[32mVal: Epoch = 44 | Loss 1.817 | Accuracy = 47.414\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 45 | Loss = 1.643 | Accuracy = 49.375\n",
            "\u001b[32mVal: Epoch = 45 | Loss 1.811 | Accuracy = 49.138\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 46 | Loss = 1.512 | Accuracy = 55.937\n",
            "\u001b[32mVal: Epoch = 46 | Loss 1.816 | Accuracy = 50.862\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 47 | Loss = 1.554 | Accuracy = 54.062\n",
            "\u001b[32mVal: Epoch = 47 | Loss 1.816 | Accuracy = 50.862\n",
            "\u001b[30mTrain: Epoch = 48 | Loss = 1.469 | Accuracy = 58.438\n",
            "\u001b[32mVal: Epoch = 48 | Loss 1.806 | Accuracy = 50.000\n",
            "\u001b[30mTrain: Epoch = 49 | Loss = 1.512 | Accuracy = 55.625\n",
            "\u001b[32mVal: Epoch = 49 | Loss 1.790 | Accuracy = 50.862\n",
            "\u001b[30mTrain: Epoch = 50 | Loss = 1.634 | Accuracy = 49.688\n",
            "\u001b[32mVal: Epoch = 50 | Loss 1.775 | Accuracy = 50.000\n",
            "\u001b[30mTrain: Epoch = 51 | Loss = 1.644 | Accuracy = 52.500\n",
            "\u001b[32mVal: Epoch = 51 | Loss 1.781 | Accuracy = 49.138\n",
            "\u001b[30mTrain: Epoch = 52 | Loss = 1.581 | Accuracy = 58.125\n",
            "\u001b[32mVal: Epoch = 52 | Loss 1.802 | Accuracy = 49.138\n",
            "\u001b[30mTrain: Epoch = 53 | Loss = 1.552 | Accuracy = 54.375\n",
            "\u001b[32mVal: Epoch = 53 | Loss 1.825 | Accuracy = 48.276\n",
            "\u001b[30mTrain: Epoch = 54 | Loss = 1.601 | Accuracy = 52.500\n",
            "\u001b[32mVal: Epoch = 54 | Loss 1.826 | Accuracy = 46.552\n",
            "\u001b[30mTrain: Epoch = 55 | Loss = 1.625 | Accuracy = 55.000\n",
            "\u001b[32mVal: Epoch = 55 | Loss 1.808 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 56 | Loss = 1.640 | Accuracy = 48.750\n",
            "\u001b[32mVal: Epoch = 56 | Loss 1.795 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 57 | Loss = 1.550 | Accuracy = 53.125\n",
            "\u001b[32mVal: Epoch = 57 | Loss 1.792 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 58 | Loss = 1.744 | Accuracy = 49.688\n",
            "\u001b[32mVal: Epoch = 58 | Loss 1.790 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 59 | Loss = 1.497 | Accuracy = 57.188\n",
            "\u001b[32mVal: Epoch = 59 | Loss 1.780 | Accuracy = 49.138\n",
            "\u001b[30mTrain: Epoch = 60 | Loss = 1.542 | Accuracy = 54.062\n",
            "\u001b[32mVal: Epoch = 60 | Loss 1.796 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 61 | Loss = 1.508 | Accuracy = 53.750\n",
            "\u001b[32mVal: Epoch = 61 | Loss 1.791 | Accuracy = 46.552\n",
            "\u001b[30mTrain: Epoch = 62 | Loss = 1.620 | Accuracy = 52.812\n",
            "\u001b[32mVal: Epoch = 62 | Loss 1.781 | Accuracy = 46.552\n",
            "\u001b[30mTrain: Epoch = 63 | Loss = 1.594 | Accuracy = 49.375\n",
            "\u001b[32mVal: Epoch = 63 | Loss 1.768 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 64 | Loss = 1.414 | Accuracy = 59.375\n",
            "\u001b[32mVal: Epoch = 64 | Loss 1.766 | Accuracy = 50.000\n",
            "\u001b[30mTrain: Epoch = 65 | Loss = 1.505 | Accuracy = 54.688\n",
            "\u001b[32mVal: Epoch = 65 | Loss 1.756 | Accuracy = 50.000\n",
            "\u001b[30mTrain: Epoch = 66 | Loss = 1.394 | Accuracy = 57.188\n",
            "\u001b[32mVal: Epoch = 66 | Loss 1.769 | Accuracy = 49.138\n",
            "\u001b[30mTrain: Epoch = 67 | Loss = 1.528 | Accuracy = 56.250\n",
            "\u001b[32mVal: Epoch = 67 | Loss 1.776 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 68 | Loss = 1.548 | Accuracy = 53.438\n",
            "\u001b[32mVal: Epoch = 68 | Loss 1.793 | Accuracy = 45.690\n",
            "\u001b[30mTrain: Epoch = 69 | Loss = 1.502 | Accuracy = 56.250\n",
            "\u001b[32mVal: Epoch = 69 | Loss 1.815 | Accuracy = 46.552\n",
            "\u001b[30mTrain: Epoch = 70 | Loss = 1.451 | Accuracy = 61.562\n",
            "\u001b[32mVal: Epoch = 70 | Loss 1.810 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 71 | Loss = 1.398 | Accuracy = 61.875\n",
            "\u001b[32mVal: Epoch = 71 | Loss 1.783 | Accuracy = 48.276\n",
            "\u001b[30mTrain: Epoch = 72 | Loss = 1.522 | Accuracy = 56.875\n",
            "\u001b[32mVal: Epoch = 72 | Loss 1.761 | Accuracy = 48.276\n",
            "\u001b[30mTrain: Epoch = 73 | Loss = 1.523 | Accuracy = 57.500\n",
            "\u001b[32mVal: Epoch = 73 | Loss 1.762 | Accuracy = 49.138\n",
            "\u001b[30mTrain: Epoch = 74 | Loss = 1.419 | Accuracy = 58.438\n",
            "\u001b[32mVal: Epoch = 74 | Loss 1.786 | Accuracy = 48.276\n",
            "\u001b[30mTrain: Epoch = 75 | Loss = 1.458 | Accuracy = 55.937\n",
            "\u001b[32mVal: Epoch = 75 | Loss 1.773 | Accuracy = 50.000\n",
            "\u001b[30mTrain: Epoch = 76 | Loss = 1.507 | Accuracy = 55.625\n",
            "\u001b[32mVal: Epoch = 76 | Loss 1.773 | Accuracy = 50.000\n",
            "\u001b[30mTrain: Epoch = 77 | Loss = 1.436 | Accuracy = 55.312\n",
            "\u001b[32mVal: Epoch = 77 | Loss 1.773 | Accuracy = 50.000\n",
            "\u001b[30mTrain: Epoch = 78 | Loss = 1.442 | Accuracy = 58.750\n",
            "\u001b[32mVal: Epoch = 78 | Loss 1.774 | Accuracy = 50.000\n",
            "\u001b[30mTrain: Epoch = 79 | Loss = 1.445 | Accuracy = 58.750\n",
            "\u001b[32mVal: Epoch = 79 | Loss 1.774 | Accuracy = 49.138\n",
            "\u001b[30mTrain: Epoch = 80 | Loss = 1.437 | Accuracy = 57.500\n",
            "\u001b[32mVal: Epoch = 80 | Loss 1.774 | Accuracy = 49.138\n",
            "\u001b[30mTrain: Epoch = 81 | Loss = 1.459 | Accuracy = 55.000\n",
            "\u001b[32mVal: Epoch = 81 | Loss 1.775 | Accuracy = 49.138\n",
            "\u001b[30mTrain: Epoch = 82 | Loss = 1.429 | Accuracy = 59.375\n",
            "\u001b[32mVal: Epoch = 82 | Loss 1.777 | Accuracy = 48.276\n",
            "\u001b[30mTrain: Epoch = 83 | Loss = 1.452 | Accuracy = 56.875\n",
            "\u001b[32mVal: Epoch = 83 | Loss 1.778 | Accuracy = 49.138\n",
            "\u001b[30mTrain: Epoch = 84 | Loss = 1.514 | Accuracy = 54.688\n",
            "\u001b[32mVal: Epoch = 84 | Loss 1.780 | Accuracy = 49.138\n",
            "\u001b[30mTrain: Epoch = 85 | Loss = 1.514 | Accuracy = 53.125\n",
            "\u001b[32mVal: Epoch = 85 | Loss 1.780 | Accuracy = 48.276\n",
            "\u001b[30mTrain: Epoch = 86 | Loss = 1.485 | Accuracy = 54.375\n",
            "\u001b[32mVal: Epoch = 86 | Loss 1.781 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 87 | Loss = 1.458 | Accuracy = 56.250\n",
            "\u001b[32mVal: Epoch = 87 | Loss 1.781 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 88 | Loss = 1.411 | Accuracy = 59.062\n",
            "\u001b[32mVal: Epoch = 88 | Loss 1.780 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 89 | Loss = 1.456 | Accuracy = 55.937\n",
            "\u001b[32mVal: Epoch = 89 | Loss 1.779 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 90 | Loss = 1.464 | Accuracy = 58.750\n",
            "\u001b[32mVal: Epoch = 90 | Loss 1.777 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 91 | Loss = 1.375 | Accuracy = 58.438\n",
            "\u001b[32mVal: Epoch = 91 | Loss 1.775 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 92 | Loss = 1.418 | Accuracy = 62.187\n",
            "\u001b[32mVal: Epoch = 92 | Loss 1.774 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 93 | Loss = 1.468 | Accuracy = 56.875\n",
            "\u001b[32mVal: Epoch = 93 | Loss 1.774 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 94 | Loss = 1.440 | Accuracy = 57.812\n",
            "\u001b[32mVal: Epoch = 94 | Loss 1.773 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 95 | Loss = 1.390 | Accuracy = 63.750\n",
            "\u001b[32mVal: Epoch = 95 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 96 | Loss = 1.407 | Accuracy = 60.312\n",
            "\u001b[32mVal: Epoch = 96 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 97 | Loss = 1.370 | Accuracy = 65.000\n",
            "\u001b[32mVal: Epoch = 97 | Loss 1.770 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 98 | Loss = 1.425 | Accuracy = 58.750\n",
            "\u001b[32mVal: Epoch = 98 | Loss 1.768 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 99 | Loss = 1.448 | Accuracy = 55.000\n",
            "\u001b[32mVal: Epoch = 99 | Loss 1.766 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 100 | Loss = 1.516 | Accuracy = 54.062\n",
            "\u001b[32mVal: Epoch = 100 | Loss 1.765 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 101 | Loss = 1.474 | Accuracy = 57.188\n",
            "\u001b[32mVal: Epoch = 101 | Loss 1.765 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 102 | Loss = 1.429 | Accuracy = 59.375\n",
            "\u001b[32mVal: Epoch = 102 | Loss 1.764 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 103 | Loss = 1.468 | Accuracy = 57.500\n",
            "\u001b[32mVal: Epoch = 103 | Loss 1.763 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 104 | Loss = 1.478 | Accuracy = 59.688\n",
            "\u001b[32mVal: Epoch = 104 | Loss 1.764 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 105 | Loss = 1.477 | Accuracy = 55.937\n",
            "\u001b[32mVal: Epoch = 105 | Loss 1.767 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 106 | Loss = 1.492 | Accuracy = 55.937\n",
            "\u001b[32mVal: Epoch = 106 | Loss 1.769 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 107 | Loss = 1.465 | Accuracy = 57.812\n",
            "\u001b[32mVal: Epoch = 107 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 108 | Loss = 1.440 | Accuracy = 56.563\n",
            "\u001b[32mVal: Epoch = 108 | Loss 1.774 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 109 | Loss = 1.456 | Accuracy = 56.875\n",
            "\u001b[32mVal: Epoch = 109 | Loss 1.776 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 110 | Loss = 1.459 | Accuracy = 58.125\n",
            "\u001b[32mVal: Epoch = 110 | Loss 1.777 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 111 | Loss = 1.442 | Accuracy = 55.312\n",
            "\u001b[32mVal: Epoch = 111 | Loss 1.776 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 112 | Loss = 1.390 | Accuracy = 59.375\n",
            "\u001b[32mVal: Epoch = 112 | Loss 1.774 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 113 | Loss = 1.413 | Accuracy = 57.500\n",
            "\u001b[32mVal: Epoch = 113 | Loss 1.770 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 114 | Loss = 1.446 | Accuracy = 58.125\n",
            "\u001b[32mVal: Epoch = 114 | Loss 1.769 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 115 | Loss = 1.464 | Accuracy = 57.188\n",
            "\u001b[32mVal: Epoch = 115 | Loss 1.768 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 116 | Loss = 1.360 | Accuracy = 57.188\n",
            "\u001b[32mVal: Epoch = 116 | Loss 1.768 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 117 | Loss = 1.464 | Accuracy = 55.625\n",
            "\u001b[32mVal: Epoch = 117 | Loss 1.767 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 118 | Loss = 1.424 | Accuracy = 61.875\n",
            "\u001b[32mVal: Epoch = 118 | Loss 1.769 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 119 | Loss = 1.430 | Accuracy = 58.125\n",
            "\u001b[32mVal: Epoch = 119 | Loss 1.769 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 120 | Loss = 1.524 | Accuracy = 55.312\n",
            "\u001b[32mVal: Epoch = 120 | Loss 1.770 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 121 | Loss = 1.499 | Accuracy = 58.125\n",
            "\u001b[32mVal: Epoch = 121 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 122 | Loss = 1.488 | Accuracy = 54.375\n",
            "\u001b[32mVal: Epoch = 122 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 123 | Loss = 1.377 | Accuracy = 59.375\n",
            "\u001b[32mVal: Epoch = 123 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 124 | Loss = 1.430 | Accuracy = 55.312\n",
            "\u001b[32mVal: Epoch = 124 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 125 | Loss = 1.509 | Accuracy = 56.250\n",
            "\u001b[32mVal: Epoch = 125 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 126 | Loss = 1.348 | Accuracy = 58.750\n",
            "\u001b[32mVal: Epoch = 126 | Loss 1.776 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 127 | Loss = 1.491 | Accuracy = 56.875\n",
            "\u001b[32mVal: Epoch = 127 | Loss 1.777 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 128 | Loss = 1.473 | Accuracy = 61.875\n",
            "\u001b[32mVal: Epoch = 128 | Loss 1.776 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 129 | Loss = 1.494 | Accuracy = 57.188\n",
            "\u001b[32mVal: Epoch = 129 | Loss 1.775 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 130 | Loss = 1.514 | Accuracy = 56.563\n",
            "\u001b[32mVal: Epoch = 130 | Loss 1.774 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 131 | Loss = 1.380 | Accuracy = 60.938\n",
            "\u001b[32mVal: Epoch = 131 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 132 | Loss = 1.462 | Accuracy = 56.563\n",
            "\u001b[32mVal: Epoch = 132 | Loss 1.770 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 133 | Loss = 1.402 | Accuracy = 58.438\n",
            "\u001b[32mVal: Epoch = 133 | Loss 1.770 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 134 | Loss = 1.449 | Accuracy = 61.250\n",
            "\u001b[32mVal: Epoch = 134 | Loss 1.770 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 135 | Loss = 1.480 | Accuracy = 56.250\n",
            "\u001b[32mVal: Epoch = 135 | Loss 1.770 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 136 | Loss = 1.358 | Accuracy = 62.500\n",
            "\u001b[32mVal: Epoch = 136 | Loss 1.770 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 137 | Loss = 1.394 | Accuracy = 56.250\n",
            "\u001b[32mVal: Epoch = 137 | Loss 1.769 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 138 | Loss = 1.536 | Accuracy = 55.937\n",
            "\u001b[32mVal: Epoch = 138 | Loss 1.769 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 139 | Loss = 1.420 | Accuracy = 56.875\n",
            "\u001b[32mVal: Epoch = 139 | Loss 1.769 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 140 | Loss = 1.338 | Accuracy = 63.750\n",
            "\u001b[32mVal: Epoch = 140 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 141 | Loss = 1.480 | Accuracy = 53.438\n",
            "\u001b[32mVal: Epoch = 141 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 142 | Loss = 1.359 | Accuracy = 61.250\n",
            "\u001b[32mVal: Epoch = 142 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 143 | Loss = 1.537 | Accuracy = 54.062\n",
            "\u001b[32mVal: Epoch = 143 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 144 | Loss = 1.381 | Accuracy = 60.000\n",
            "\u001b[32mVal: Epoch = 144 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 145 | Loss = 1.375 | Accuracy = 62.813\n",
            "\u001b[32mVal: Epoch = 145 | Loss 1.768 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 146 | Loss = 1.425 | Accuracy = 58.750\n",
            "\u001b[32mVal: Epoch = 146 | Loss 1.769 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 147 | Loss = 1.495 | Accuracy = 59.375\n",
            "\u001b[32mVal: Epoch = 147 | Loss 1.769 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 148 | Loss = 1.279 | Accuracy = 61.562\n",
            "\u001b[32mVal: Epoch = 148 | Loss 1.768 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 149 | Loss = 1.421 | Accuracy = 57.500\n",
            "\u001b[32mVal: Epoch = 149 | Loss 1.769 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 150 | Loss = 1.476 | Accuracy = 54.375\n",
            "\u001b[32mVal: Epoch = 150 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 151 | Loss = 1.480 | Accuracy = 58.750\n",
            "\u001b[32mVal: Epoch = 151 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 152 | Loss = 1.449 | Accuracy = 58.750\n",
            "\u001b[32mVal: Epoch = 152 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 153 | Loss = 1.381 | Accuracy = 62.813\n",
            "\u001b[32mVal: Epoch = 153 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 154 | Loss = 1.318 | Accuracy = 60.000\n",
            "\u001b[32mVal: Epoch = 154 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 155 | Loss = 1.451 | Accuracy = 57.188\n",
            "\u001b[32mVal: Epoch = 155 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 156 | Loss = 1.392 | Accuracy = 64.062\n",
            "\u001b[32mVal: Epoch = 156 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 157 | Loss = 1.370 | Accuracy = 60.000\n",
            "\u001b[32mVal: Epoch = 157 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 158 | Loss = 1.426 | Accuracy = 58.438\n",
            "\u001b[32mVal: Epoch = 158 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 159 | Loss = 1.518 | Accuracy = 55.937\n",
            "\u001b[32mVal: Epoch = 159 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 160 | Loss = 1.363 | Accuracy = 58.438\n",
            "\u001b[32mVal: Epoch = 160 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 161 | Loss = 1.383 | Accuracy = 60.312\n",
            "\u001b[32mVal: Epoch = 161 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 162 | Loss = 1.378 | Accuracy = 57.812\n",
            "\u001b[32mVal: Epoch = 162 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 163 | Loss = 1.436 | Accuracy = 57.812\n",
            "\u001b[32mVal: Epoch = 163 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 164 | Loss = 1.435 | Accuracy = 58.750\n",
            "\u001b[32mVal: Epoch = 164 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 165 | Loss = 1.345 | Accuracy = 60.938\n",
            "\u001b[32mVal: Epoch = 165 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 166 | Loss = 1.422 | Accuracy = 55.937\n",
            "\u001b[32mVal: Epoch = 166 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 167 | Loss = 1.390 | Accuracy = 59.062\n",
            "\u001b[32mVal: Epoch = 167 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 168 | Loss = 1.432 | Accuracy = 58.438\n",
            "\u001b[32mVal: Epoch = 168 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 169 | Loss = 1.326 | Accuracy = 60.312\n",
            "\u001b[32mVal: Epoch = 169 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 170 | Loss = 1.396 | Accuracy = 59.375\n",
            "\u001b[32mVal: Epoch = 170 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 171 | Loss = 1.417 | Accuracy = 57.500\n",
            "\u001b[32mVal: Epoch = 171 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 172 | Loss = 1.484 | Accuracy = 56.563\n",
            "\u001b[32mVal: Epoch = 172 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 173 | Loss = 1.479 | Accuracy = 56.563\n",
            "\u001b[32mVal: Epoch = 173 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 174 | Loss = 1.332 | Accuracy = 58.125\n",
            "\u001b[32mVal: Epoch = 174 | Loss 1.773 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 175 | Loss = 1.414 | Accuracy = 59.375\n",
            "\u001b[32mVal: Epoch = 175 | Loss 1.773 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 176 | Loss = 1.433 | Accuracy = 59.375\n",
            "\u001b[32mVal: Epoch = 176 | Loss 1.773 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 177 | Loss = 1.427 | Accuracy = 55.625\n",
            "\u001b[32mVal: Epoch = 177 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 178 | Loss = 1.520 | Accuracy = 58.125\n",
            "\u001b[32mVal: Epoch = 178 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 179 | Loss = 1.359 | Accuracy = 64.062\n",
            "\u001b[32mVal: Epoch = 179 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 180 | Loss = 1.384 | Accuracy = 57.812\n",
            "\u001b[32mVal: Epoch = 180 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 181 | Loss = 1.350 | Accuracy = 58.438\n",
            "\u001b[32mVal: Epoch = 181 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 182 | Loss = 1.399 | Accuracy = 58.438\n",
            "\u001b[32mVal: Epoch = 182 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 183 | Loss = 1.500 | Accuracy = 55.937\n",
            "\u001b[32mVal: Epoch = 183 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 184 | Loss = 1.389 | Accuracy = 60.312\n",
            "\u001b[32mVal: Epoch = 184 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 185 | Loss = 1.401 | Accuracy = 59.688\n",
            "\u001b[32mVal: Epoch = 185 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 186 | Loss = 1.468 | Accuracy = 58.750\n",
            "\u001b[32mVal: Epoch = 186 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 187 | Loss = 1.448 | Accuracy = 57.812\n",
            "\u001b[32mVal: Epoch = 187 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 188 | Loss = 1.477 | Accuracy = 57.812\n",
            "\u001b[32mVal: Epoch = 188 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 189 | Loss = 1.465 | Accuracy = 59.062\n",
            "\u001b[32mVal: Epoch = 189 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 190 | Loss = 1.385 | Accuracy = 60.625\n",
            "\u001b[32mVal: Epoch = 190 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 191 | Loss = 1.471 | Accuracy = 57.500\n",
            "\u001b[32mVal: Epoch = 191 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 192 | Loss = 1.419 | Accuracy = 58.438\n",
            "\u001b[32mVal: Epoch = 192 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 193 | Loss = 1.279 | Accuracy = 62.813\n",
            "\u001b[32mVal: Epoch = 193 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 194 | Loss = 1.425 | Accuracy = 59.062\n",
            "\u001b[32mVal: Epoch = 194 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 195 | Loss = 1.458 | Accuracy = 54.688\n",
            "\u001b[32mVal: Epoch = 195 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 196 | Loss = 1.254 | Accuracy = 62.813\n",
            "\u001b[32mVal: Epoch = 196 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 197 | Loss = 1.411 | Accuracy = 59.688\n",
            "\u001b[32mVal: Epoch = 197 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 198 | Loss = 1.496 | Accuracy = 55.312\n",
            "\u001b[32mVal: Epoch = 198 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 199 | Loss = 1.396 | Accuracy = 60.938\n",
            "\u001b[32mVal: Epoch = 199 | Loss 1.772 | Accuracy = 47.414\n",
            "\u001b[30mTrain: Epoch = 200 | Loss = 1.413 | Accuracy = 59.688\n",
            "\u001b[32mVal: Epoch = 200 | Loss 1.771 | Accuracy = 47.414\n",
            "\u001b[36mBest Acc -->  50.86206896551724\n",
            "\u001b[36mLast Acc -->  47.41379310344828\n"
          ]
        }
      ],
      "source": [
        "train_iter = 0\n",
        "val_iter = 0\n",
        "min_accuracy = 0\n",
        "\n",
        "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
        "val_samples = len(test_dataset)\n",
        "iterPerEpoch = len(train_loader)\n",
        "val_steps = len(val_loader)\n",
        "cudnn.benchmark\n",
        "model_checkpoint = \"model\" #name\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    numCorrTrain = 0\n",
        "\n",
        "    #blocks to train\n",
        "    if homework_step > 0:\n",
        "        model.lstm_cell.train(True)\n",
        "    model.classifier.train(True)\n",
        "\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        train_iter += 1\n",
        "        optimizer_fn.zero_grad()\n",
        "\n",
        "        # (BS, Frames, C, W, H) --> (Frames, BS, C, W, H)\n",
        "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "        labelVariable = targets.to(DEVICE)\n",
        "\n",
        "        # feeds in model\n",
        "        output_label, _ = model(inputVariable)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_fn(output_label, labelVariable)\n",
        "\n",
        "        # backward loss and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "\n",
        "        #compute the training accuracy\n",
        "        _, predicted = torch.max(output_label.data, 1)\n",
        "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
        "    #train_logger.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
        "    print(Fore.BLACK + 'Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch+1, avg_loss, trainAccuracy))\n",
        "    if validate:\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            model.train(False)\n",
        "            val_loss_epoch = 0\n",
        "            numCorr = 0\n",
        "            for j, (inputs, targets) in enumerate(val_loader):\n",
        "                val_iter += 1\n",
        "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "                labelVariable = targets.to(DEVICE)\n",
        "\n",
        "                output_label, _ = model(inputVariable)\n",
        "                val_loss = loss_fn(output_label, labelVariable)\n",
        "                val_loss_step = val_loss.data.item()\n",
        "                val_loss_epoch += val_loss_step\n",
        "                _, predicted = torch.max(output_label.data, 1)\n",
        "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
        "                #val_logger.add_step_data(val_iter, numCorr, val_loss_step)\n",
        "\n",
        "            val_accuracy = (numCorr / val_samples) * 100\n",
        "            avg_val_loss = val_loss_epoch / val_steps\n",
        "\n",
        "            print(Fore.GREEN + 'Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "            if val_accuracy > min_accuracy:\n",
        "                print(\"[||| NEW BEST on val||||]\")\n",
        "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "                min_accuracy = val_accuracy\n",
        "\n",
        "    optim_scheduler.step()\n",
        "\n",
        "print(Fore.CYAN + \"Best Acc --> \", min_accuracy)\n",
        "print(Fore.CYAN + \"Last Acc --> \", val_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrLs_T2Qd0kc"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "gqK1ExB0cl8D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss 1.771 | Accuracy = 47.414\n"
          ]
        }
      ],
      "source": [
        "model.train(False)\n",
        "val_loss_epoch = 0\n",
        "numCorr = 0\n",
        "val_iter = 0\n",
        "val_samples = len(test_dataset)\n",
        "val_steps = len(val_loader)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for j, (inputs, targets) in enumerate(val_loader):\n",
        "        val_iter += 1\n",
        "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "        labelVariable = targets.to(DEVICE)\n",
        "\n",
        "        output_label, _ = model(inputVariable)\n",
        "        val_loss = loss_fn(output_label, labelVariable)\n",
        "        val_loss_step = val_loss.data.item()\n",
        "        val_loss_epoch += val_loss_step\n",
        "        _, predicted = torch.max(output_label.data, 1)\n",
        "        numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
        "\n",
        "    val_accuracy = (numCorr / val_samples) * 100\n",
        "    avg_val_loss = val_loss_epoch / val_steps\n",
        "\n",
        "print('Loss {:.3f} | Accuracy = {:.3f}'.format(avg_val_loss, val_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr9BxL8zeBfv"
      },
      "source": [
        "#**Learning with Temporal information** (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-qHYgnyf_wn"
      },
      "source": [
        "#**Learning with Spatio-Temporal information** (ConvLSTM)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Kk0rtAlnSlDF",
        "jiwWzjzSio-h",
        "g1GznJhObXPk",
        "Sy4KrHClbAmC",
        "UPwkOR8taVdN",
        "Ru8vllrMbgvL",
        "z0MWgLingzhw",
        "lrLs_T2Qd0kc",
        "Gr9BxL8zeBfv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
